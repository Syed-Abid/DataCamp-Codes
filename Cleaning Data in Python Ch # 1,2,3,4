1. Print the information of ride_sharing.
2. Use .describe() to print the summary statistics of the user_type column from ride_sharing.

# Print the information of ride_sharing
print(ride_sharing.info())

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe())

3. Convert user_type into categorical by assigning it the 'category' data type and store it in the user_type_cat column.
4. Make sure you converted user_type_cat correctly by using an assert statement.

# Print the information of ride_sharing
print(ride_sharing.info())

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe())

# Convert user_type from integer to category
ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')

# Write an assert statement confirming the change
assert ride_sharing['user_type_cat'].dtype == 'category'

# Print new summary statistics 
print(ride_sharing['user_type_cat'].describe())

5. Use the .strip() method to strip duration of "minutes" and store it in the duration_trim column.
6. Convert duration_trim to int and store it in the duration_time column.
7. Write an assert statement that checks if duration_time's data type is now an int.
8. Print the average ride duration.

# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes') 

# Convert duration to integer
ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')

# Write an assert statement making sure of conversion
assert ride_sharing['duration_time'].dtype == 'int'

# Print formed columns and calculate average ride duration 
print(ride_sharing[['duration','duration_trim','duration_time']])
print(ride_sharing['duration_time'].mean())


9. Convert the tire_sizes column from category to 'int'.
10. Use .loc[] to set all values of tire_sizes above 27 to 27.
11. Reconvert back tire_sizes to 'category' from int.
12. Print the description of the tire_sizes.

# Convert tire_sizes to integer
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

# Set all values above 27 to 27
ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27

# Reconvert tire_sizes back to categorical
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')

# Print tire size description
print(ride_sharing['tire_sizes'].describe())

13. Convert ride_date to a datetime object using to_datetime(), then convert the datetime object into a date and store it in ride_dt column.
14. Create the variable today, which stores today's date by using the dt.date.today() function.
15. For all instances of ride_dt in the future, set them to today's date.
16. Print the maximum date in the ride_dt column.

# Convert ride_date to date
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date

# Save today's date
today = dt.date.today()

# Set all in the future to today's date
ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today

# Print maximum of ride_dt column
print(ride_sharing['ride_dt'].max())


17. Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.
18. Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.
19. Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order.

# Find duplicates
duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)

# Sort your duplicated rides
duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')

# Print relevant columns
print(duplicated_rides[['ride_id','duration','user_birth_year']])

20. Drop complete duplicates in ride_sharing and store the results in ride_dup.
21. Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.
22. Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.
23. Find duplicates again and run the assert statement to verify de-duplication.

# Drop complete duplicates from ride_sharing
ride_dup = ride_sharing.drop_duplicates()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': 'min', 'duration': 'mean'}

# Group by ride_id and compute new statistics
ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()

# Find duplicated values again
duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]

# Assert duplicates are processed
assert duplicated_rides.shape[0] == 0


24. Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.
25. Print the unique values of the survey columns in airlines using the .unique() method.

# Print categories DataFrame
print(categories)

# Print unique values of survey columns in airlines
print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
print('Safety: ', airlines['safety'].unique(), "\n")
print('Satisfaction: ', airlines['satisfaction'].unique(), "\n")

26. Create a set out of the cleanliness column in airlines using set() and find the inconsistent
category by finding the difference in the cleanliness column of categories.
27. Find rows of airlines with a cleanliness value not in categories and print the output.

# Find the cleanliness category in airlines not in categories
cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])

# Find rows with that category
cat_clean_rows = airlines['cleanliness'].isin(cat_clean)

# Print rows with inconsistent category
print(airlines[cat_clean_rows])

28. Print the rows with the consistent categories of cleanliness only.
# Print rows with consistent categories only
print(airlines[~cat_clean_rows])

29. Print the unique values in dest_region and dest_size respectively.
# Print unique values of both columns
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

30. Change the capitalization of all values of dest_region to lowercase.
31. Replace the 'eur' with 'europe' in dest_region using the .replace() method.

# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower()
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

32. Strip white spaces from the dest_size column using the .strip() method.
33. Verify that the changes have been into effect by printing the unique values of the columns using .unique() .

# Remove white spaces from `dest_size`
airlines['dest_size'] = airlines['dest_size'].str.strip()

# Verify changes have been effected
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

34. Create the ranges and labels for the wait_type column mentioned in the description.
35. Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.
36. Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.
37. Create the day_week column by using .replace().

# Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, 
                                labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)


38. Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.
39. Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.
40. Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'.

# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[acct_eu, 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'

41. Print the header of account_opened from the banking DataFrame and take a look at the different results.
# Print the header of account_opened
print(banking['account_opened'].head())

42. Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.
# Print the header of account_opened
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce') 
                                           
43. Extract the year from the amended account_opened column and assign it to the acct_year column.
44. Print the newly created acct_year column.
# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])


