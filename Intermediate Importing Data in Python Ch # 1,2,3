1. Import the function urlretrieve from the subpackage urllib.request.
2. Assign the URL of the file to the variable url.
3. Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.
4. Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell.

# Import package
from urllib.request import urlretrieve

# Import pandas
import pandas as pd

# Assign url of file: url
url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url,'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())

5. Assign the URL of the file to the variable url.
6. Read file into a DataFrame df using pd.read_csv(), recalling that the separator in the file is ';'.
7. Print the head of the DataFrame df.
8. Execute the rest of the code to plot histogram of the first feature in the DataFrame df.

# Import packages
import matplotlib.pyplot as plt
import pandas as pd

# Assign url of file: url
url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url,sep=';')

# Print the head of the DataFrame
print(df.head())

# Plot first column of df
df.iloc[:, 0].hist()
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()


9. Assign the URL of the file to the variable url.
10. Read the file in url into a dictionary xls using pd.read_excel() recalling that, in order to import all sheets you need to pass None to the argument sheet_name.
11. Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.
12. Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'

# Import package
import pandas as pd

# Assign url of file: url
url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xls
xls = pd.read_excel(url,sheet_name=None)

# Print the sheetnames to the shell
print(xls.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xls['1700'].head())


13. Import the functions urlopen and Request from the subpackage urllib.request.
14. Package the request to the url "https://campus.datacamp.com/courses/1606/4135?ex=2" using the function Request() and assign it to request.
15. Send the request and catch the response in the variable response with the function urlopen().
16. Run the rest of the code to see the datatype of response and to close the connection!

# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request: request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Print the datatype of response
print(type(response))

# Be polite and close the response!
response.close()


17. Send the request and catch the response in the variable response with the function urlopen(), as in the previous exercise.
18. Extract the response using the read() method and store the result in the variable html.
19. Print the string html.
20. Hit submit to perform all of the above and to close the response: be tidy!

# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Extract the response: html
html = response.read()

# Print the html
print(html)

# Be polite and close the response!
response.close()

21. Import the package requests.
22. Assign the URL of interest to the variable url.
23. Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.
24. Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable text.
25. Hit submit to print the HTML of the webpage.

# Import package
import requests

# Specify the url: url
url = "http://www.datacamp.com/teach/documentation"

# Packages the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response: text
text = r.text

# Print the html
print(text)

26. Import the function BeautifulSoup from the package bs4.
27. Assign the URL of interest to the variable url.
28. Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.
29. Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable html_doc.
30. Create a BeautifulSoup object soup from the resulting HTML using the function BeautifulSoup().
31. Use the method prettify() on soup and assign the result to pretty_soup.
32. Hit submit to print to prettified HTML to your shell!

# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Prettify the BeautifulSoup object: pretty_soup
pretty_soup = soup.prettify()

# Print the response
print(pretty_soup)

33. In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup()
and to assign the resulting soup to the variable soup.
34. Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.
35. Print the title of Guido's webpage to the shell using the print() function.
36. Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.

# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Get the title of Guido's webpage: guido_title
guido_title = soup.title

# Print the title of Guido's webpage to the shell
print(guido_title)

# Get Guido's text: guido_text
guido_text = soup.get_text()

# Print Guido's text to the shell
print(guido_text)

37. Use the method find_all() to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag <a> but passed to find_all() 
without angle brackets; store the result in the variable a_tags.
38. The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; 
to do this, for every element link in a_tags, you want to print() link.get('href').
39. Hit submit to print the text from Guido's webpage to the shell.

# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Print the title of Guido's webpage
print(soup.title)

# Find all 'a' tags (which define hyperlinks): a_tags
a_tags = soup.find_all('a')

# Print the URLs to the shell
for link in a_tags:
    print(link.get('href'))
    
40. Load the JSON 'a_movie.json' into the variable json_data within the context provided by the with statement. To do so, use the function json.load() within the 
context manager.
41. Use a for loop to print all key-value pairs in the dictionary json_data. Recall that you can access a value in a dictionary using the syntax: dictionary[key].

# Load JSON: json_data
with open("a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])
    
42. Import the requests package.
43. Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. 
The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.
44. Print the text of the response object r by using its text attribute and passing the result to the print() function.

# Import requests package
import requests

# Assign URL to variable: url
url = 'http://www.omdbapi.com?apikey=72bc447a&t=the+social+network'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Print the text of the response
print(r.text)


45. Pass the variable url to the requests.get() function in order to send the relevant request and catch the response, 
assigning the resultant response message to the variable r.
46. Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.
47. Hit submit to print the key-value pairs of the dictionary json_data to the shell.

# Import package
import requests

# Assign URL to variable: url
url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


48. Assign the relevant URL to the variable url.
49. Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.
50. The variable pizza_extract holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function print() to print this string to the shell.

# Import package
import requests

# Assign URL to variable: url
url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)

51. Create your Stream object with the credentials given.
52. Filter your Stream variable for the keywords "clinton", "trump", "sanders", and "cruz".

# Store credentials in relevant variables
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"

# Create your Stream object with credentials
stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)

# Filter your Stream variable
stream.filter(track=['clinton','trump','sanders','cruz'])

53. Assign the filename 'tweets.txt' to the variable tweets_data_path.
54. Initialize tweets_data as an empty list to store the tweets in.
55. Within the for loop initiated by for line in tweets_file:, load each tweet into a variable, tweet, using json.loads(), 
then append tweet to tweets_data using the append() method.
56. Hit submit and check out the keys of the first tweet dictionary printed to the shell.

# Import package
import json

# String of path to file: tweets_data_path
tweets_data_path = 'tweets.txt'

# Initialize empty list to store tweets: tweets_data
tweets_data = []

# Open connection to file
tweets_file = open(tweets_data_path, "r")

# Read in tweets and store in list: tweets_data
for line in tweets_file:
    tweet = json.loads(line)
    tweets_data.append(tweet)

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())


57. Use pd.DataFrame() to construct a DataFrame of tweet texts and languages; to do so, the first argument should be tweets_data, a list of dictionaries. 
The second argument to pd.DataFrame() is a list of the keys you wish to have as columns. Assign the result of the pd.DataFrame() call to df.
58. Print the head of the DataFrame.

# Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=['text','lang'])

# Print head of DataFrame
print(df.head())


59. Within the for loop for index, row in df.iterrows():, the code currently increases the value of clinton by 1 each time a tweet (text row) 
mentioning 'Clinton' is encountered; complete the code so that the same happens for trump, sanders and cruz.

# Initialize list to store tweet counts
[clinton, trump, sanders, cruz] = [0, 0, 0, 0]

# Iterate through df, counting the number of tweets in which
# each candidate is mentioned
for index, row in df.iterrows():
    clinton += word_in_text('clinton', row['text'])
    trump += word_in_text('trump', row['text'])
    sanders += word_in_text('sanders', row['text'])
    cruz += word_in_text('cruz', row['text'])


60. Import both matplotlib.pyplot and seaborn using the aliases plt and sns, respectively.
61. Complete the arguments of sns.barplot:
62. The first argument should be the list of labels to appear on the x-axis (created in the previous step).
63. The second argument should be a list of the variables you wish to plot, as produced in the previous exercise (i.e. a list containing clinton, trump, etc).

# Import packages
import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(color_codes=True)

# Create a list of labels:cd
cd = ['clinton', 'trump', 'sanders', 'cruz']

# Plot the bar chart
ax = sns.barplot(cd, [clinton,trump,sanders,cruz])
ax.set(ylabel="count")
plt.show()


